{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca25057",
   "metadata": {},
   "source": [
    "## IMPORT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b54a64c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelli SpaCy italiano e inglese caricati con successo\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "\n",
    "# Carica il modello multilingua di SpaCy\n",
    "try:\n",
    "    nlp_it = spacy.load(\"it_core_news_sm\")\n",
    "    nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Modelli SpaCy italiano e inglese caricati con successo\")\n",
    "    nlp_available = True\n",
    "except Exception as e:\n",
    "    print(f\"ATTENZIONE: Errore nel caricamento dei modelli SpaCy: {e}\")\n",
    "    print(\"Installa con:\")\n",
    "    print(\"  python -m spacy download it_core_news_sm\")\n",
    "    print(\"  python -m spacy download en_core_web_sm\")\n",
    "    print(\"  pip install langdetect\")\n",
    "    nlp_available = False\n",
    "    nlp_it = None\n",
    "    nlp_en = None\n",
    "    nlp = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44489094",
   "metadata": {},
   "source": [
    "## MODELLO RICONOSCIMENTO LINGUAGGIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "23e19563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp_model(text):\n",
    "    \"\"\"Rileva la lingua e restituisce il modello appropriato\"\"\"\n",
    "    try:\n",
    "        lang = detect(text[:500])  # Usa i primi 500 caratteri\n",
    "        return nlp_it if lang == 'it' else nlp_en\n",
    "    except:\n",
    "        return nlp_it  # Default italiano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9f9ee",
   "metadata": {},
   "source": [
    "## DATASET LOADS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1b8b2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"./data/\"\n",
    "artists = pd.read_csv(f'{data}artists.csv', sep=';', index_col=0)\n",
    "tracks = pd.read_csv(f'{data}tracks.csv', sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a1661",
   "metadata": {},
   "source": [
    "## ARTISTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c053af0",
   "metadata": {},
   "source": [
    "### DROP LONGITUDE, LATITUDE AND ACTIVE_END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3afdfef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di colonne prima del drop: 13\n",
      "Colonne: ['name', 'gender', 'birth_date', 'birth_place', 'nationality', 'description', 'active_start', 'active_end', 'province', 'region', 'country', 'latitude', 'longitude']\n",
      "\n",
      "Numero di colonne dopo il drop: 10\n",
      "Colonne rimosse: ['longitude', 'latitude', 'active_end']\n",
      "Colonne rimanenti: ['name', 'gender', 'birth_date', 'birth_place', 'nationality', 'description', 'active_start', 'province', 'region', 'country']\n"
     ]
    }
   ],
   "source": [
    "# Print number of columns before dropping\n",
    "print(f'Numero di colonne prima del drop: {len(artists.columns)}')\n",
    "print(f'Colonne: {list(artists.columns)}')\n",
    "\n",
    "columns_to_drop = ['longitude', 'latitude', 'active_end']\n",
    "artists = artists.drop(columns=columns_to_drop)\n",
    "\n",
    "# Print number of columns after dropping\n",
    "print(f'\\nNumero di colonne dopo il drop: {len(artists.columns)}')\n",
    "print(f'Colonne rimosse: {columns_to_drop}')\n",
    "print(f'Colonne rimanenti: {list(artists.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aacbdd",
   "metadata": {},
   "source": [
    "## TRACK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938189f",
   "metadata": {},
   "source": [
    "### Drop no lyrics songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "781b6040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tracce rimosse: 0\n",
      "Tracce rimanenti: 11163 (100.00% del totale originale)\n"
     ]
    }
   ],
   "source": [
    "# Drop tracks with null lyrics\n",
    "\n",
    "# Statistiche prima del drop\n",
    "initial_count = len(tracks)\n",
    "null_lyrics_count = tracks['lyrics'].isna().sum()\n",
    "\n",
    "tracks = tracks.dropna(subset=['lyrics'])\n",
    "\n",
    "# Statistiche dopo il drop\n",
    "final_count = len(tracks)\n",
    "dropped_count = initial_count - final_count\n",
    "print(f'\\nTracce rimosse: {dropped_count}')\n",
    "print(f'Tracce rimanenti: {final_count} ({(final_count/initial_count)*100:.2f}% del totale originale)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594a078",
   "metadata": {},
   "source": [
    "### FIX OUT OF RANGE YEARS OF THE TRACK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0a98ee25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valori fuori range (<1992 o >2025): 2152 su 11163 (19.28%)\n",
      "\n",
      "Statistiche dopo la pulizia:\n",
      "count         8573.0\n",
      "mean     2015.151172\n",
      "std          7.04169\n",
      "min           1992.0\n",
      "25%           2011.0\n",
      "50%           2016.0\n",
      "75%           2021.0\n",
      "max           2025.0\n",
      "Name: year, dtype: Float64\n",
      "\n",
      "Numero di righe con year vuoto dopo pulizia: 2590 su 11163 (23.20%)\n"
     ]
    }
   ],
   "source": [
    "# Fix out-of-range years in tracks: impostare a vuoto se year < 1992 o > 2025\n",
    "col = 'year'\n",
    "total = len(tracks)\n",
    "\n",
    "# Convertiamo in numerico\n",
    "years = pd.to_numeric(tracks[col], errors='coerce')\n",
    "\n",
    "# Identifichiamo valori fuori range\n",
    "mask_out = (years < 1992) | (years > 2025)\n",
    "out_count = int(mask_out.sum())\n",
    "print(f'Valori fuori range (<1992 o >2025): {out_count} su {total} ({(out_count/total)*100:.2f}%)')\n",
    "\n",
    "# Impostiamo a vuoto (pd.NA) i valori fuori range\n",
    "tracks.loc[mask_out, col] = pd.NA\n",
    "\n",
    "# Convertiamo la colonna in Int64 nullable per mantenere tipo numerico con NA\n",
    "tracks[col] = pd.to_numeric(tracks[col], errors='coerce').astype('Int64')\n",
    "\n",
    "# Statistiche dopo la pulizia\n",
    "years_after = pd.to_numeric(tracks[col], errors='coerce')\n",
    "print('\\nStatistiche dopo la pulizia:')\n",
    "print(years_after.describe())\n",
    "\n",
    "# Numero di righe con year vuoto dopo la pulizia\n",
    "final_empty = tracks[col].isna().sum()\n",
    "print(f'\\nNumero di righe con year vuoto dopo pulizia: {final_empty} su {total} ({(final_empty/total)*100:.2f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e49fff",
   "metadata": {},
   "source": [
    "### FIX MISSING LYRICS STATISTICS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b188cf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FIX n_sentences ---\n",
      "Valori mancanti prima: 0 (0.00%)\n",
      "Valori fixati: 0\n",
      "Valori mancanti dopo: 0 (0.00%)\n",
      "\n",
      "--- FIX n_tokens ---\n",
      "Valori mancanti prima: 0 (0.00%)\n",
      "Valori fixati: 0\n",
      "Valori mancanti dopo: 0 (0.00%)\n",
      "\n",
      "--- FIX tokens_per_sent ---\n",
      "Valori mancanti prima: 0 (0.00%)\n",
      "Valori fixati: 0\n",
      "Valori mancanti dopo: 0 (0.00%)\n",
      "\n",
      "--- FIX char_per_tok ---\n",
      "Valori mancanti prima: 0 (0.00%)\n",
      "Valori fixati: 0\n",
      "Valori mancanti dopo: 0 (0.00%)\n",
      "\n",
      "--- FIX lexical_density ---\n",
      "Valori mancanti prima: 0 (0.00%)\n",
      "Valori fixati: 0\n",
      "Valori mancanti dopo: 0 (0.00%)\n",
      "\n",
      "--- FIX avg_token_per_clause ---\n",
      "Valori mancanti prima: 73 (0.65%)\n",
      "Elaborazione di 73 righe con SpaCy...\n",
      "Elaborazione canzoni italiane...\n",
      "Elaborazione canzoni inglesi...\n",
      "\n",
      "Valori fixati: 73\n",
      "Valori mancanti dopo: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FIX n_sentences\n",
    "# ============================================================================\n",
    "print(\"\\n--- FIX n_sentences ---\")\n",
    "missing_before = tracks['n_sentences'].isna().sum()\n",
    "print(f\"Valori mancanti prima: {missing_before} ({(missing_before/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "# Identifica righe con n_sentences mancante\n",
    "mask_missing = tracks['n_sentences'].isna()\n",
    "\n",
    "# Calcola n_sentences dalle lyrics\n",
    "for idx in tracks[mask_missing].index:\n",
    "    lyrics = tracks.loc[idx, 'lyrics']\n",
    "    # Split su punti, esclamativi, interrogativi\n",
    "    sentences = re.split(r'[.!?]+', str(lyrics))\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    n_sent = len(sentences)\n",
    "    tracks.loc[idx, 'n_sentences'] = n_sent\n",
    "    \n",
    "# Statistiche dopo il fix\n",
    "missing_after = tracks['n_sentences'].isna().sum()\n",
    "fixed = missing_before - missing_after\n",
    "print(f\"Valori fixati: {fixed}\")\n",
    "print(f\"Valori mancanti dopo: {missing_after} ({(missing_after/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX n_tokens\n",
    "# ============================================================================\n",
    "print(\"\\n--- FIX n_tokens ---\")\n",
    "missing_before = tracks['n_tokens'].isna().sum()\n",
    "print(f\"Valori mancanti prima: {missing_before} ({(missing_before/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "# Identifica righe con n_tokens mancante\n",
    "mask_missing = tracks['n_tokens'].isna()\n",
    "\n",
    "# Calcola n_tokens dalle lyrics\n",
    "for idx in tracks[mask_missing].index:\n",
    "    lyrics = tracks.loc[idx, 'lyrics']\n",
    "    # Estrai parole (token)\n",
    "    tokens = re.findall(r'\\b\\w+\\b', str(lyrics).lower())\n",
    "    n_tok = len(tokens)\n",
    "    tracks.loc[idx, 'n_tokens'] = n_tok\n",
    "    \n",
    "# Statistiche dopo il fix\n",
    "missing_after = tracks['n_tokens'].isna().sum()\n",
    "fixed = missing_before - missing_after\n",
    "print(f\"Valori fixati: {fixed}\")\n",
    "print(f\"Valori mancanti dopo: {missing_after} ({(missing_after/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX tokens_per_sent\n",
    "# ============================================================================\n",
    "print(\"\\n--- FIX tokens_per_sent ---\")\n",
    "missing_before = tracks['tokens_per_sent'].isna().sum()\n",
    "print(f\"Valori mancanti prima: {missing_before} ({(missing_before/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "# Identifica righe con tokens_per_sent mancante\n",
    "mask_missing = tracks['tokens_per_sent'].isna()\n",
    "\n",
    "# Calcola tokens_per_sent da n_tokens e n_sentences\n",
    "for idx in tracks[mask_missing].index:\n",
    "    n_tok = tracks.loc[idx, 'n_tokens']\n",
    "    n_sent = tracks.loc[idx, 'n_sentences']\n",
    "    \n",
    "    if pd.notna(n_tok) and pd.notna(n_sent) and n_sent > 0:\n",
    "        tok_per_sent = n_tok / n_sent\n",
    "        tracks.loc[idx, 'tokens_per_sent'] = tok_per_sent\n",
    "    \n",
    "# Statistiche dopo il fix\n",
    "missing_after = tracks['tokens_per_sent'].isna().sum()\n",
    "fixed = missing_before - missing_after\n",
    "print(f\"Valori fixati: {fixed}\")\n",
    "print(f\"Valori mancanti dopo: {missing_after} ({(missing_after/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX char_per_tok\n",
    "# ============================================================================\n",
    "print(\"\\n--- FIX char_per_tok ---\")\n",
    "missing_before = tracks['char_per_tok'].isna().sum()\n",
    "print(f\"Valori mancanti prima: {missing_before} ({(missing_before/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "# Identifica righe con char_per_tok mancante\n",
    "mask_missing = tracks['char_per_tok'].isna()\n",
    "\n",
    "# Calcola char_per_tok dalle lyrics\n",
    "for idx in tracks[mask_missing].index:\n",
    "    lyrics = tracks.loc[idx, 'lyrics']\n",
    "    # Estrai token e calcola media caratteri\n",
    "    tokens = re.findall(r'\\b\\w+\\b', str(lyrics).lower())\n",
    "    n_tok = len(tokens)\n",
    "    \n",
    "    if n_tok > 0:\n",
    "        total_chars = sum(len(t) for t in tokens)\n",
    "        char_per_tok = total_chars / n_tok\n",
    "        tracks.loc[idx, 'char_per_tok'] = char_per_tok\n",
    "    \n",
    "# Statistiche dopo il fix\n",
    "missing_after = tracks['char_per_tok'].isna().sum()\n",
    "fixed = missing_before - missing_after\n",
    "print(f\"Valori fixati: {fixed}\")\n",
    "print(f\"Valori mancanti dopo: {missing_after} ({(missing_after/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX lexical_density\n",
    "# ============================================================================\n",
    "print(\"\\n--- FIX lexical_density ---\")\n",
    "missing_before = tracks['lexical_density'].isna().sum()\n",
    "print(f\"Valori mancanti prima: {missing_before} ({(missing_before/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "# Identifica righe con lexical_density mancante\n",
    "mask_missing = tracks['lexical_density'].isna()\n",
    "\n",
    "# Calcola lexical_density dalle lyrics\n",
    "for idx in tracks[mask_missing].index:\n",
    "    lyrics = tracks.loc[idx, 'lyrics']\n",
    "    # Estrai token e calcola densitÃ  lessicale\n",
    "    tokens = re.findall(r'\\b\\w+\\b', str(lyrics).lower())\n",
    "    n_tok = len(tokens)\n",
    "    \n",
    "    if n_tok > 0:\n",
    "        unique_tokens = len(set(tokens))\n",
    "        lex_dens = unique_tokens / n_tok\n",
    "        tracks.loc[idx, 'lexical_density'] = lex_dens\n",
    "    \n",
    "# Statistiche dopo il fix\n",
    "missing_after = tracks['lexical_density'].isna().sum()\n",
    "fixed = missing_before - missing_after\n",
    "print(f\"Valori fixati: {fixed}\")\n",
    "print(f\"Valori mancanti dopo: {missing_after} ({(missing_after/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX avg_token_per_clause\n",
    "# ============================================================================\n",
    "print(\"\\n--- FIX avg_token_per_clause ---\")\n",
    "\n",
    "if not nlp_available or nlp_it is None or nlp_en is None:\n",
    "    print(\"ERRORE: Modelli SpaCy non disponibili. Salta il calcolo di avg_token_per_clause\")\n",
    "else:\n",
    "    missing_before = tracks['avg_token_per_clause'].isna().sum() if 'avg_token_per_clause' in tracks.columns else len(tracks)\n",
    "    print(f\"Valori mancanti prima: {missing_before} ({(missing_before/len(tracks)*100):.2f}%)\")\n",
    "    \n",
    "    if 'avg_token_per_clause' not in tracks.columns:\n",
    "        tracks['avg_token_per_clause'] = pd.NA\n",
    "        print(\"Colonna 'avg_token_per_clause' creata\")\n",
    "    \n",
    "    mask_missing = tracks['avg_token_per_clause'].isna()\n",
    "    indices_to_process = tracks[mask_missing].index.tolist()\n",
    "    total_to_process = len(indices_to_process)\n",
    "    \n",
    "    print(f\"Elaborazione di {total_to_process} righe con SpaCy...\")\n",
    "    \n",
    "    # Prepara i dati: [(idx, lyrics, lang), ...]\n",
    "    data_to_process = []\n",
    "    for idx in indices_to_process:\n",
    "        lyrics = tracks.loc[idx, 'lyrics']\n",
    "        if pd.notna(lyrics) and lyrics != '':\n",
    "            try:\n",
    "                lang = detect(str(lyrics)[:500])\n",
    "            except:\n",
    "                lang = 'it'\n",
    "            data_to_process.append((idx, str(lyrics)[:1000000], lang))\n",
    "    \n",
    "    # Elabora in batch per lingua\n",
    "    print(\"Elaborazione canzoni italiane...\")\n",
    "    italian_data = [(idx, text) for idx, text, lang in data_to_process if lang == 'it']\n",
    "    for i, (idx, lyrics) in enumerate(italian_data):\n",
    "        try:\n",
    "            doc = nlp_it(lyrics)\n",
    "            n_clauses = sum(1 for _ in doc.sents)\n",
    "            for sent in doc.sents:\n",
    "                for token in sent:\n",
    "                    if token.pos_ == 'VERB' and token.dep_ in ['ccomp', 'xcomp', 'advcl', 'relcl', 'acl', 'csubj', 'csubjpass']:\n",
    "                        n_clauses += 1\n",
    "            n_tokens = len([t for t in doc if t.is_alpha])\n",
    "            if n_clauses > 0:\n",
    "                tracks.loc[idx, 'avg_token_per_clause'] = n_tokens / n_clauses\n",
    "            \n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(f\"  Processate {i + 1}/{len(italian_data)} canzoni italiane...\")\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(\"Elaborazione canzoni inglesi...\")\n",
    "    english_data = [(idx, text) for idx, text, lang in data_to_process if lang != 'it']\n",
    "    for i, (idx, lyrics) in enumerate(english_data):\n",
    "        try:\n",
    "            doc = nlp_en(lyrics)\n",
    "            n_clauses = sum(1 for _ in doc.sents)\n",
    "            for sent in doc.sents:\n",
    "                for token in sent:\n",
    "                    if token.pos_ == 'VERB' and token.dep_ in ['ccomp', 'xcomp', 'advcl', 'relcl', 'acl', 'csubj', 'csubjpass']:\n",
    "                        n_clauses += 1\n",
    "            n_tokens = len([t for t in doc if t.is_alpha])\n",
    "            if n_clauses > 0:\n",
    "                tracks.loc[idx, 'avg_token_per_clause'] = n_tokens / n_clauses\n",
    "            \n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(f\"  Processate {i + 1}/{len(english_data)} canzoni inglesi...\")\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    missing_after = tracks['avg_token_per_clause'].isna().sum()\n",
    "    fixed = missing_before - missing_after\n",
    "    print(f\"\\nValori fixati: {fixed}\")\n",
    "    print(f\"Valori mancanti dopo: {missing_after} ({(missing_after/len(tracks)*100):.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fd1bf",
   "metadata": {},
   "source": [
    "## SAVE CHANGES ON CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb2479b",
   "metadata": {},
   "source": [
    "### ARTISTS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artists dataset\n",
    "output_path = f'{data}artists.csv'\n",
    "artists.to_csv(output_path, sep=';')\n",
    "print(f'Dataset tracks salvato in: {output_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a109afb",
   "metadata": {},
   "source": [
    "### TRACKS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b405dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tracks salvato in: ./data/tracks.csv\n"
     ]
    }
   ],
   "source": [
    "# Save tracks dataset\n",
    "output_path = f'{data}tracks.csv'\n",
    "tracks.to_csv(output_path, sep=',')\n",
    "print(f'Dataset tracks salvato in: {output_path}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
