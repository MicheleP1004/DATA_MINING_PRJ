{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ca25057",
   "metadata": {},
   "source": [
    "## IMPORT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b54a64c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelli SpaCy italiano e inglese caricati con successo\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import spacy\n",
    "from langdetect import detect\n",
    "\n",
    "# Carica il modello multilingua di SpaCy\n",
    "try:\n",
    "    nlp_it = spacy.load(\"it_core_news_sm\")\n",
    "    nlp_en = spacy.load(\"en_core_web_sm\")\n",
    "    print(\"Modelli SpaCy italiano e inglese caricati con successo\")\n",
    "    nlp_available = True\n",
    "except Exception as e:\n",
    "    print(f\"ATTENZIONE: Errore nel caricamento dei modelli SpaCy: {e}\")\n",
    "    print(\"Installa con:\")\n",
    "    print(\"  python -m spacy download it_core_news_sm\")\n",
    "    print(\"  python -m spacy download en_core_web_sm\")\n",
    "    print(\"  pip install langdetect\")\n",
    "    nlp_available = False\n",
    "    nlp_it = None\n",
    "    nlp_en = None\n",
    "    nlp = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44489094",
   "metadata": {},
   "source": [
    "## MODELLO RICONOSCIMENTO LINGUAGGIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23e19563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp_model(text):\n",
    "    \"\"\"Rileva la lingua e restituisce il modello appropriato\"\"\"\n",
    "    try:\n",
    "        lang = detect(text[:500])  # Usa i primi 500 caratteri\n",
    "        return nlp_it if lang == 'it' else nlp_en\n",
    "    except:\n",
    "        return nlp_it  # Default italiano"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca9f9ee",
   "metadata": {},
   "source": [
    "## DATASET LOADS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1b8b2a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"./data/\"\n",
    "artists = pd.read_csv(f'{data}artists.csv', sep=',', index_col=0)\n",
    "tracks = pd.read_csv(f'{data}tracks.csv', sep=',', index_col=0)\n",
    "\n",
    "# ### Uncomment the following lines to backup at raw data files\n",
    "# data = \"./raw_data/\"\n",
    "# artists = pd.read_csv(f'{data}raw_artists.csv', sep=',', index_col=0)\n",
    "# tracks = pd.read_csv(f'{data}raw_tracks.csv', sep=',', index_col=0)\n",
    "# output_path = \"./data/\"\n",
    "# artists.to_csv(f'{data}raw_artists.csv', sep=',')\n",
    "# tracks.to_csv(f'{data}raw_tracks.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b08a1661",
   "metadata": {},
   "source": [
    "## ARTISTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c053af0",
   "metadata": {},
   "source": [
    "### DROP LONGITUDE, LATITUDE AND ACTIVE_END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "3afdfef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero di colonne prima del drop: 13\n",
      "Colonne: ['name', 'gender', 'birth_date', 'birth_place', 'nationality', 'description', 'active_start', 'active_end', 'province', 'region', 'country', 'latitude', 'longitude']\n",
      "\n",
      "Numero di colonne dopo il drop: 10\n",
      "Colonne rimosse: ['longitude', 'latitude', 'active_end']\n",
      "Colonne rimanenti: ['name', 'gender', 'birth_date', 'birth_place', 'nationality', 'description', 'active_start', 'province', 'region', 'country']\n"
     ]
    }
   ],
   "source": [
    "# Print number of columns before dropping\n",
    "print(f'Numero di colonne prima del drop: {len(artists.columns)}')\n",
    "print(f'Colonne: {list(artists.columns)}')\n",
    "\n",
    "columns_to_drop = ['longitude', 'latitude', 'active_end']\n",
    "artists = artists.drop(columns=columns_to_drop)\n",
    "\n",
    "# Print number of columns after dropping\n",
    "print(f'\\nNumero di colonne dopo il drop: {len(artists.columns)}')\n",
    "print(f'Colonne rimosse: {columns_to_drop}')\n",
    "print(f'Colonne rimanenti: {list(artists.columns)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aacbdd",
   "metadata": {},
   "source": [
    "## TRACK\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3909c1",
   "metadata": {},
   "source": [
    "### STANDARDIZE ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9369fd52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID con underscore (TR_numero): 73\n",
      "Totale ID: 11166\n",
      "\n",
      "ID da standardizzare: 73\n",
      "Standardizzazione completata\n",
      "\n",
      "Mapping salvato in: ./data/temp/id_standardization_mapping.csv\n",
      "\n",
      "=== VERIFICA FINALE ===\n",
      "ID con underscore rimasti: 0\n",
      "Formato ID dopo standardizzazione: ['TR934808', 'TR760029', 'TR916821', 'TR480968', 'TR585039']\n"
     ]
    }
   ],
   "source": [
    "# standardizzo la scrittura degli id, alcuni hanno il _ cioè TR_numero e altri TRnumero\n",
    "\n",
    "# Conta quanti ID hanno l'underscore\n",
    "ids_with_underscore = tracks.index.astype(str).str.contains('_').sum()\n",
    "print(f\"ID con underscore (TR_numero): {ids_with_underscore}\")\n",
    "print(f\"Totale ID: {len(tracks)}\")\n",
    "\n",
    "# Crea un mapping vecchio_id → nuovo_id\n",
    "id_standardization = {}\n",
    "for idx in tracks.index:\n",
    "    idx_str = str(idx)\n",
    "    if '_' in idx_str:\n",
    "        # Rimuovi l'underscore: TR_123 → TR123\n",
    "        new_id = idx_str.replace('_', '')\n",
    "        id_standardization[idx] = new_id\n",
    "\n",
    "print(f\"\\nID da standardizzare: {len(id_standardization)}\")\n",
    "\n",
    "# Applica la standardizzazione\n",
    "if len(id_standardization) > 0:\n",
    "    for old_id, new_id in id_standardization.items():\n",
    "        tracks.rename(index={old_id: new_id}, inplace=True)\n",
    "    \n",
    "    print(f\"Standardizzazione completata\")\n",
    "\n",
    "    # Salva il mapping\n",
    "    std_mapping_df = pd.DataFrame(\n",
    "        list(id_standardization.items()), \n",
    "        columns=['old_id', 'new_id']\n",
    "    )\n",
    "    std_mapping_df.to_csv('./data/temp/id_standardization_mapping.csv', index=False)\n",
    "    print(f\"\\nMapping salvato in: ./data/temp/id_standardization_mapping.csv\")\n",
    "else:\n",
    "    print(\"Nessun ID da standardizzare - tutti gli ID sono già nel formato corretto\")\n",
    "\n",
    "# Verifica finale\n",
    "print(f\"\\n=== VERIFICA FINALE ===\")\n",
    "remaining_underscores = tracks.index.astype(str).str.contains('_').sum()\n",
    "print(f\"ID con underscore rimasti: {remaining_underscores}\")\n",
    "print(f\"Formato ID dopo standardizzazione: {tracks.index[:5].tolist()}\")\n",
    "# tracks.to_csv(\"./data/temp/idpatch.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9cc900",
   "metadata": {},
   "source": [
    "### REMOVE THE DUPLICATES OF THE SONGS WITH SAME ID AND SAME TITLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1853f461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Righe duplicate rimosse ===\n",
      "            id                                full_title  n_missing  temp_uid\n",
      "4628  TR367132  BUGIE by Madame (Ft. Carl Brave & Rkomi)          0  TR367132\n",
      "4648  TR367132                        ​sentimi by Madame          2  TR367132\n"
     ]
    }
   ],
   "source": [
    "# Copia di sicurezza\n",
    "df = tracks.copy()\n",
    "\n",
    "# Porta l'indice in colonna, crea identificatore univoco temporaneo\n",
    "df = df.reset_index(drop=False)\n",
    "index_col = df.columns[0]  # il nome della colonna creata dal reset_index\n",
    "df.rename(columns={index_col: 'temp_uid'}, inplace=True)\n",
    "\n",
    "# Se la colonna id non esiste, creala dai valori originali dell'indice\n",
    "if 'id' not in df.columns:\n",
    "    df['id'] = df['temp_uid']\n",
    "\n",
    "# Conta valori mancanti per riga\n",
    "df['n_missing'] = df.isna().sum(axis=1)\n",
    "\n",
    "# Ordina per id, full_title, numero di missing (meno NaN prima)\n",
    "df.sort_values(by=['id', 'full_title', 'n_missing'], \n",
    "               ascending=[True, True, True], \n",
    "               inplace=True, kind='stable')\n",
    "\n",
    "# Trova duplicati veri (stesso id + full_title)\n",
    "dup_mask = df.duplicated(subset=['id', 'full_title'], keep='first')\n",
    "\n",
    "# Righe duplicate da rimuovere\n",
    "removed_rows = df[dup_mask].copy()\n",
    "\n",
    "# Righe duplicate che saranno tenute\n",
    "kept_rows = df[~dup_mask].copy()\n",
    "\n",
    "print(\"=== Righe duplicate rimosse ===\")\n",
    "print(removed_rows[['id', 'full_title', 'n_missing', 'temp_uid']])\n",
    "\n",
    "# Rimuove i duplicati dal dataframe\n",
    "df_clean = df[~dup_mask].copy()\n",
    "df_clean.drop(columns=['n_missing'], inplace=True)\n",
    "\n",
    "# Reimposta l'indice sul temp_uid\n",
    "df_clean.set_index('temp_uid', inplace=True)\n",
    "\n",
    "# Aggiorna il dataframe principale\n",
    "tracks = df_clean.copy()\n",
    "# removed_rows.to_csv(\"./data/temp/removed_duplicates_same_id_fulltitle.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1a8bd8",
   "metadata": {},
   "source": [
    "### FIX THE DIFFERENT TRACK THAT HAVE SAME ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e1f11c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STEP 2: RIASSEGNAZIONE ID DUPLICATI RIMANENTI ---\n",
      "Numero di id duplicati: 71\n",
      "Numero totale di righe con ID duplicati: 142\n",
      "\n",
      "→ Riassegnati 71 ID duplicati.\n",
      "→ Range nuovi ID: TR999927 - TR999997\n",
      "→ Mapping salvato in ./data/temp/id_duplicates_mapping.csv\n",
      "\n",
      "=== VERIFICA FINALE ===\n",
      "Righe totali: 11164\n",
      "ID univoci: 11164\n",
      "ID duplicati residui: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- STEP 2: RIASSEGNAZIONE ID DUPLICATI RIMANENTI ---\")\n",
    "print(\"Numero di id duplicati:\", tracks.shape[0] - tracks.index.unique().size)\n",
    "\n",
    "# Reset index temporaneamente per lavorare con posizioni\n",
    "tracks_reset = tracks.reset_index()\n",
    "id_col = tracks_reset.columns[0]  # Prima colonna è l'ex-indice\n",
    "\n",
    "# Identifica ID duplicati\n",
    "duplicated_mask = tracks_reset[id_col].duplicated(keep=False)\n",
    "num_duplicates = duplicated_mask.sum()\n",
    "print(f\"Numero totale di righe con ID duplicati: {num_duplicates}\")\n",
    "\n",
    "if num_duplicates > 0:\n",
    "    # Trova massimo numero ID esistente\n",
    "    def extract_number(id_str):\n",
    "        m = re.search(r'TR(\\d+)', str(id_str))\n",
    "        return int(m.group(1)) if m else 0\n",
    "    \n",
    "    max_number = tracks_reset[id_col].apply(extract_number).max()\n",
    "    new_id_counter = max_number + 1\n",
    "    \n",
    "    id_mapping = {}\n",
    "    \n",
    "    # Per ogni gruppo di ID duplicati\n",
    "    for dup_id in tracks_reset[duplicated_mask][id_col].unique():\n",
    "        # Trova tutte le righe con questo ID (usando posizioni numeriche)\n",
    "        dup_indices = tracks_reset[tracks_reset[id_col] == dup_id].index.tolist()\n",
    "        \n",
    "        # print(f\"\\nProcessando ID duplicato: {dup_id}\")\n",
    "        # print(f\"  Trovate {len(dup_indices)} occorrenze\")\n",
    "        \n",
    "        # Mantieni il primo, riassegna gli altri\n",
    "        for i, row_idx in enumerate(dup_indices):\n",
    "            if i > 0:  # Dalla seconda occorrenza in poi\n",
    "                new_id = f\"TR{new_id_counter}\"\n",
    "                old_id = tracks_reset.loc[row_idx, id_col]\n",
    "                \n",
    "                # Aggiorna l'ID nella riga specifica\n",
    "                tracks_reset.loc[row_idx, id_col] = new_id\n",
    "                \n",
    "                # Salva il mapping\n",
    "                id_mapping[old_id] = new_id\n",
    "                \n",
    "                # print(f\"  Riga {row_idx}: {old_id} → {new_id}\")\n",
    "                new_id_counter += 1\n",
    "    \n",
    "    print(f\"\\n→ Riassegnati {len(id_mapping)} ID duplicati.\")\n",
    "    print(f\"→ Range nuovi ID: TR{max_number + 1} - TR{new_id_counter - 1}\")\n",
    "    \n",
    "    # Salva mapping\n",
    "    mapping_df = pd.DataFrame(list(id_mapping.items()), columns=['old_id', 'new_id'])\n",
    "    mapping_df.to_csv(\"./data/temp/id_duplicates_mapping.csv\", index=False)\n",
    "    print(\"→ Mapping salvato in ./data/temp/id_duplicates_mapping.csv\")\n",
    "    \n",
    "    # Reimposta l'indice\n",
    "    tracks = tracks_reset.set_index(id_col)\n",
    "    tracks.index.name = None\n",
    "    \n",
    "else:\n",
    "    print(\"Nessun ID duplicato da riassegnare.\")\n",
    "\n",
    "# Verifica finale\n",
    "print(f\"\\n=== VERIFICA FINALE ===\")\n",
    "print(f\"Righe totali: {len(tracks)}\")\n",
    "print(f\"ID univoci: {tracks.index.nunique()}\")\n",
    "print(f\"ID duplicati residui: {tracks.index.duplicated().sum()}\")\n",
    "\n",
    "# Mostra gli ID duplicati rimasti (se ce ne sono)\n",
    "if tracks.index.duplicated().sum() > 0:\n",
    "    print(\"\\nID duplicati ancora presenti\")\n",
    "    remaining_dups = tracks.index[tracks.index.duplicated(keep=False)].unique()\n",
    "    print(f\"ID duplicati rimasti: {remaining_dups.tolist()}\")\n",
    "    \n",
    "# tracks.to_csv(\"./data/temp/idfixed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5938189f",
   "metadata": {},
   "source": [
    "### Drop no lyrics songs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "781b6040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tracce rimosse: 3\n",
      "Tracce rimanenti: 11161 (99.97% del totale originale)\n"
     ]
    }
   ],
   "source": [
    "# Drop tracks with null lyrics\n",
    "\n",
    "# Statistiche prima del drop\n",
    "initial_count = len(tracks)\n",
    "null_lyrics_count = tracks['lyrics'].isna().sum()\n",
    "\n",
    "tracks = tracks.dropna(subset=['lyrics'])\n",
    "\n",
    "# Statistiche dopo il drop\n",
    "final_count = len(tracks)\n",
    "dropped_count = initial_count - final_count\n",
    "print(f'\\nTracce rimosse: {dropped_count}')\n",
    "print(f'Tracce rimanenti: {final_count} ({(final_count/initial_count)*100:.2f}% del totale originale)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1855b8c4",
   "metadata": {},
   "source": [
    "### DROP FAKE LYRICS (2 PERCENTILE WITH LESS WORDS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "eaf7236f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracce rimosse: 221\n",
      "Tracce rimanenti: 10940\n"
     ]
    }
   ],
   "source": [
    "fake_lyrics1 = tracks[tracks['n_tokens'] < 65]\n",
    "# fake_lyrics2 = tracks[tracks['n_sentences'] > 73]\n",
    "fake_lyrics = pd.concat([fake_lyrics1])\n",
    "fake_lyrics[['title','lyrics','n_tokens']].to_csv('./data/temp/fake_lyrics.csv', sep=',')\n",
    "# Rimuovi le tracce fake dal dataframe tracks\n",
    "tracks = tracks.drop(fake_lyrics.index)\n",
    "\n",
    "print(f'Tracce rimosse: {fake_lyrics.shape[0]}')\n",
    "print(f'Tracce rimanenti: {tracks.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2594a078",
   "metadata": {},
   "source": [
    "### FIX OUT OF RANGE YEARS OF THE TRACK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0a98ee25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valori fuori range (<1992 o >2025): 2113 su 10940 (19.31%)\n",
      "\n",
      "Statistiche dopo la pulizia:\n",
      "count        8413.0\n",
      "mean     2015.26673\n",
      "std        6.951512\n",
      "min          1992.0\n",
      "25%          2011.0\n",
      "50%          2016.0\n",
      "75%          2021.0\n",
      "max          2025.0\n",
      "Name: year, dtype: Float64\n",
      "\n",
      "Numero di righe con year vuoto dopo pulizia: 2527 su 10940 (23.10%)\n"
     ]
    }
   ],
   "source": [
    "# Fix out-of-range years in tracks: impostare a vuoto se year < 1992 o > 2025\n",
    "col = 'year'\n",
    "total = len(tracks)\n",
    "\n",
    "# Convertiamo in numerico\n",
    "years = pd.to_numeric(tracks[col], errors='coerce')\n",
    "\n",
    "# Identifichiamo valori fuori range\n",
    "mask_out = (years < 1992) | (years > 2025)\n",
    "out_count = int(mask_out.sum())\n",
    "print(f'Valori fuori range (<1992 o >2025): {out_count} su {total} ({(out_count/total)*100:.2f}%)')\n",
    "\n",
    "# Impostiamo a vuoto (pd.NA) i valori fuori range\n",
    "tracks.loc[mask_out, col] = pd.NA\n",
    "\n",
    "# Convertiamo la colonna in Int64 nullable per mantenere tipo numerico con NA\n",
    "tracks[col] = pd.to_numeric(tracks[col], errors='coerce').astype('Int64')\n",
    "\n",
    "# Statistiche dopo la pulizia\n",
    "years_after = pd.to_numeric(tracks[col], errors='coerce')\n",
    "print('\\nStatistiche dopo la pulizia:')\n",
    "print(years_after.describe())\n",
    "\n",
    "# Numero di righe con year vuoto dopo la pulizia\n",
    "final_empty = tracks[col].isna().sum()\n",
    "print(f'\\nNumero di righe con year vuoto dopo pulizia: {final_empty} su {total} ({(final_empty/total)*100:.2f}%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e49fff",
   "metadata": {},
   "source": [
    "### FIX MISSING LYRICS STATISTICS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "b188cf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- FIX n_sentences ---\n",
      "Valori mancanti prima: 73 (0.67%)\n",
      "Valori fixati: 73\n",
      "Valori mancanti dopo: 0 (0.00%)\n",
      "\n",
      "--- FIX n_tokens ---\n",
      "Valori mancanti prima: 73 (0.67%)\n",
      "Valori fixati: 73\n",
      "Valori mancanti dopo: 0 (0.00%)\n",
      "\n",
      "--- FIX tokens_per_sent ---\n",
      "Valori mancanti prima: 73 (0.67%)\n",
      "Valori fixati: 73\n",
      "Valori mancanti dopo: 0 (0.00%)\n",
      "\n",
      "--- FIX char_per_tok ---\n",
      "Valori mancanti prima: 73 (0.67%)\n",
      "Valori fixati: 73\n",
      "Valori mancanti dopo: 0 (0.00%)\n",
      "\n",
      "--- FIX lexical_density ---\n",
      "Valori mancanti prima: 73 (0.67%)\n",
      "Valori fixati: 73\n",
      "Valori mancanti dopo: 0 (0.00%)\n",
      "\n",
      "--- FIX avg_token_per_clause ---\n",
      "Valori mancanti prima: 73 (0.67%)\n",
      "Elaborazione di 73 righe con SpaCy...\n",
      "Elaborazione canzoni italiane...\n",
      "Elaborazione canzoni inglesi...\n",
      "\n",
      "Valori fixati: 73\n",
      "Valori mancanti dopo: 0 (0.00%)\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# FIX n_sentences\n",
    "# ============================================================================\n",
    "print(\"\\n--- FIX n_sentences ---\")\n",
    "missing_before = tracks['n_sentences'].isna().sum()\n",
    "print(f\"Valori mancanti prima: {missing_before} ({(missing_before/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "# Identifica righe con n_sentences mancante\n",
    "mask_missing = tracks['n_sentences'].isna()\n",
    "\n",
    "# Calcola n_sentences dalle lyrics\n",
    "for idx in tracks[mask_missing].index:\n",
    "    lyrics = tracks.loc[idx, 'lyrics']\n",
    "    # Split su punti, esclamativi, interrogativi\n",
    "    sentences = re.split(r'[.!?]+', str(lyrics))\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    n_sent = len(sentences)\n",
    "    tracks.loc[idx, 'n_sentences'] = n_sent\n",
    "    \n",
    "# Statistiche dopo il fix\n",
    "missing_after = tracks['n_sentences'].isna().sum()\n",
    "fixed = missing_before - missing_after\n",
    "print(f\"Valori fixati: {fixed}\")\n",
    "print(f\"Valori mancanti dopo: {missing_after} ({(missing_after/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX n_tokens\n",
    "# ============================================================================\n",
    "print(\"\\n--- FIX n_tokens ---\")\n",
    "missing_before = tracks['n_tokens'].isna().sum()\n",
    "print(f\"Valori mancanti prima: {missing_before} ({(missing_before/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "# Identifica righe con n_tokens mancante\n",
    "mask_missing = tracks['n_tokens'].isna()\n",
    "\n",
    "# Calcola n_tokens dalle lyrics\n",
    "for idx in tracks[mask_missing].index:\n",
    "    lyrics = tracks.loc[idx, 'lyrics']\n",
    "    # Estrai parole (token)\n",
    "    tokens = re.findall(r'\\b\\w+\\b', str(lyrics).lower())\n",
    "    n_tok = len(tokens)\n",
    "    tracks.loc[idx, 'n_tokens'] = n_tok\n",
    "    \n",
    "# Statistiche dopo il fix\n",
    "missing_after = tracks['n_tokens'].isna().sum()\n",
    "fixed = missing_before - missing_after\n",
    "print(f\"Valori fixati: {fixed}\")\n",
    "print(f\"Valori mancanti dopo: {missing_after} ({(missing_after/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX tokens_per_sent\n",
    "# ============================================================================\n",
    "print(\"\\n--- FIX tokens_per_sent ---\")\n",
    "missing_before = tracks['tokens_per_sent'].isna().sum()\n",
    "print(f\"Valori mancanti prima: {missing_before} ({(missing_before/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "# Identifica righe con tokens_per_sent mancante\n",
    "mask_missing = tracks['tokens_per_sent'].isna()\n",
    "\n",
    "# Calcola tokens_per_sent da n_tokens e n_sentences\n",
    "for idx in tracks[mask_missing].index:\n",
    "    n_tok = tracks.loc[idx, 'n_tokens']\n",
    "    n_sent = tracks.loc[idx, 'n_sentences']\n",
    "    \n",
    "    if pd.notna(n_tok) and pd.notna(n_sent) and n_sent > 0:\n",
    "        tok_per_sent = n_tok / n_sent\n",
    "        tracks.loc[idx, 'tokens_per_sent'] = tok_per_sent\n",
    "    \n",
    "# Statistiche dopo il fix\n",
    "missing_after = tracks['tokens_per_sent'].isna().sum()\n",
    "fixed = missing_before - missing_after\n",
    "print(f\"Valori fixati: {fixed}\")\n",
    "print(f\"Valori mancanti dopo: {missing_after} ({(missing_after/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX char_per_tok\n",
    "# ============================================================================\n",
    "print(\"\\n--- FIX char_per_tok ---\")\n",
    "missing_before = tracks['char_per_tok'].isna().sum()\n",
    "print(f\"Valori mancanti prima: {missing_before} ({(missing_before/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "# Identifica righe con char_per_tok mancante\n",
    "mask_missing = tracks['char_per_tok'].isna()\n",
    "\n",
    "# Calcola char_per_tok dalle lyrics\n",
    "for idx in tracks[mask_missing].index:\n",
    "    lyrics = tracks.loc[idx, 'lyrics']\n",
    "    # Estrai token e calcola media caratteri\n",
    "    tokens = re.findall(r'\\b\\w+\\b', str(lyrics).lower())\n",
    "    n_tok = len(tokens)\n",
    "    \n",
    "    if n_tok > 0:\n",
    "        total_chars = sum(len(t) for t in tokens)\n",
    "        char_per_tok = total_chars / n_tok\n",
    "        tracks.loc[idx, 'char_per_tok'] = char_per_tok\n",
    "    \n",
    "# Statistiche dopo il fix\n",
    "missing_after = tracks['char_per_tok'].isna().sum()\n",
    "fixed = missing_before - missing_after\n",
    "print(f\"Valori fixati: {fixed}\")\n",
    "print(f\"Valori mancanti dopo: {missing_after} ({(missing_after/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX lexical_density\n",
    "# ============================================================================\n",
    "print(\"\\n--- FIX lexical_density ---\")\n",
    "missing_before = tracks['lexical_density'].isna().sum()\n",
    "print(f\"Valori mancanti prima: {missing_before} ({(missing_before/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "# Identifica righe con lexical_density mancante\n",
    "mask_missing = tracks['lexical_density'].isna()\n",
    "\n",
    "# Calcola lexical_density dalle lyrics\n",
    "for idx in tracks[mask_missing].index:\n",
    "    lyrics = tracks.loc[idx, 'lyrics']\n",
    "    # Estrai token e calcola densità lessicale\n",
    "    tokens = re.findall(r'\\b\\w+\\b', str(lyrics).lower())\n",
    "    n_tok = len(tokens)\n",
    "    \n",
    "    if n_tok > 0:\n",
    "        unique_tokens = len(set(tokens))\n",
    "        lex_dens = unique_tokens / n_tok\n",
    "        tracks.loc[idx, 'lexical_density'] = lex_dens\n",
    "    \n",
    "# Statistiche dopo il fix\n",
    "missing_after = tracks['lexical_density'].isna().sum()\n",
    "fixed = missing_before - missing_after\n",
    "print(f\"Valori fixati: {fixed}\")\n",
    "print(f\"Valori mancanti dopo: {missing_after} ({(missing_after/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# FIX avg_token_per_clause\n",
    "# ============================================================================\n",
    "print(\"\\n--- FIX avg_token_per_clause ---\")\n",
    "\n",
    "if not nlp_available or nlp_it is None or nlp_en is None:\n",
    "    print(\"ERRORE: Modelli SpaCy non disponibili. Salta il calcolo di avg_token_per_clause\")\n",
    "else:\n",
    "    missing_before = tracks['avg_token_per_clause'].isna().sum() if 'avg_token_per_clause' in tracks.columns else len(tracks)\n",
    "    print(f\"Valori mancanti prima: {missing_before} ({(missing_before/len(tracks)*100):.2f}%)\")\n",
    "    \n",
    "    if 'avg_token_per_clause' not in tracks.columns:\n",
    "        tracks['avg_token_per_clause'] = pd.NA\n",
    "        print(\"Colonna 'avg_token_per_clause' creata\")\n",
    "    \n",
    "    mask_missing = tracks['avg_token_per_clause'].isna()\n",
    "    indices_to_process = tracks[mask_missing].index.tolist()\n",
    "    total_to_process = len(indices_to_process)\n",
    "    \n",
    "    print(f\"Elaborazione di {total_to_process} righe con SpaCy...\")\n",
    "    \n",
    "    # Prepara i dati: [(idx, lyrics, lang), ...]\n",
    "    data_to_process = []\n",
    "    for idx in indices_to_process:\n",
    "        lyrics = tracks.loc[idx, 'lyrics']\n",
    "        if pd.notna(lyrics) and lyrics != '':\n",
    "            try:\n",
    "                lang = detect(str(lyrics)[:500])\n",
    "            except:\n",
    "                lang = 'it'\n",
    "            data_to_process.append((idx, str(lyrics)[:1000000], lang))\n",
    "    \n",
    "    # Elabora in batch per lingua\n",
    "    print(\"Elaborazione canzoni italiane...\")\n",
    "    italian_data = [(idx, text) for idx, text, lang in data_to_process if lang == 'it']\n",
    "    for i, (idx, lyrics) in enumerate(italian_data):\n",
    "        try:\n",
    "            doc = nlp_it(lyrics)\n",
    "            n_clauses = sum(1 for _ in doc.sents)\n",
    "            for sent in doc.sents:\n",
    "                for token in sent:\n",
    "                    if token.pos_ == 'VERB' and token.dep_ in ['ccomp', 'xcomp', 'advcl', 'relcl', 'acl', 'csubj', 'csubjpass']:\n",
    "                        n_clauses += 1\n",
    "            n_tokens = len([t for t in doc if t.is_alpha])\n",
    "            if n_clauses > 0:\n",
    "                tracks.loc[idx, 'avg_token_per_clause'] = n_tokens / n_clauses\n",
    "            \n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(f\"  Processate {i + 1}/{len(italian_data)} canzoni italiane...\")\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    print(\"Elaborazione canzoni inglesi...\")\n",
    "    english_data = [(idx, text) for idx, text, lang in data_to_process if lang != 'it']\n",
    "    for i, (idx, lyrics) in enumerate(english_data):\n",
    "        try:\n",
    "            doc = nlp_en(lyrics)\n",
    "            n_clauses = sum(1 for _ in doc.sents)\n",
    "            for sent in doc.sents:\n",
    "                for token in sent:\n",
    "                    if token.pos_ == 'VERB' and token.dep_ in ['ccomp', 'xcomp', 'advcl', 'relcl', 'acl', 'csubj', 'csubjpass']:\n",
    "                        n_clauses += 1\n",
    "            n_tokens = len([t for t in doc if t.is_alpha])\n",
    "            if n_clauses > 0:\n",
    "                tracks.loc[idx, 'avg_token_per_clause'] = n_tokens / n_clauses\n",
    "            \n",
    "            if (i + 1) % 500 == 0:\n",
    "                print(f\"  Processate {i + 1}/{len(english_data)} canzoni inglesi...\")\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    missing_after = tracks['avg_token_per_clause'].isna().sum()\n",
    "    fixed = missing_before - missing_after\n",
    "    print(f\"\\nValori fixati: {fixed}\")\n",
    "    print(f\"Valori mancanti dopo: {missing_after} ({(missing_after/len(tracks)*100):.2f}%)\")\n",
    "\n",
    "tracks.to_csv('./data/temp/tracks_cleaned.csv', sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615bdaad",
   "metadata": {},
   "source": [
    "### CHANGE STRINGS IN FEATURED_ARTISTS TO LIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aa5c1698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== CONVERSIONE 'featured_artists' COMPLETATA ===\n",
      "Totale tracce con featured artists: 3477\n",
      "Tracce senza featured artists: 7463\n",
      "\n",
      "Esempi di conversione:\n",
      "                   title          featured_artists\n",
      "TR100258        Profondo             [Lucio Dalla]\n",
      "TR100595      FACCE VERE                    [nayt]\n",
      "TR100995   Cos’è l’amore         [Franco Califano]\n",
      "TR102300  Uno Alla Volta    [Dani Caldes, Lexotan]\n",
      "TR102302            Volo  [Patrick Benifei, Raige]\n"
     ]
    }
   ],
   "source": [
    "# === CONVERSIONE DELLA COLONNA 'featured_artists' IN LISTE DI STRINGHE ===\n",
    "\n",
    "def process_featured_artists(featured_str):\n",
    "    \"\"\"Converte una stringa di artisti separati da virgola in una lista pulita\"\"\"\n",
    "    # Se è già una lista, la restituisco così com’è\n",
    "    if isinstance(featured_str, list):\n",
    "        return featured_str\n",
    "    \n",
    "    # Se è NaN o vuoto → lista vuota\n",
    "    if pd.isna(featured_str) or featured_str == '':\n",
    "        return []\n",
    "\n",
    "    # Converte in stringa (per sicurezza)\n",
    "    featured_str = str(featured_str)\n",
    "    # Rimuove caratteri invisibili Unicode\n",
    "    featured_str = featured_str.replace('\\u200b', '').replace('\\xa0', ' ').replace('\\u202f', ' ')\n",
    "    # Splitta per virgola e pulisce spazi\n",
    "    artists_list = [artist.strip() for artist in featured_str.split(',')]\n",
    "    # Rimuove stringhe vuote\n",
    "    return [artist for artist in artists_list if artist]\n",
    "\n",
    "# Applica la conversione DIRETTAMENTE al DataFrame originale\n",
    "tracks['featured_artists'] = tracks['featured_artists'].apply(process_featured_artists)\n",
    "\n",
    "# Crea un dataframe temporaneo solo per ispezione e salvataggio\n",
    "feat_tracks = tracks[tracks['featured_artists'].apply(len) > 0].copy()\n",
    "\n",
    "# Salva solo le tracce con featured artists (per controllo o analisi)\n",
    "tracks.to_csv(f'{data}/temp/tracks_cleaned.csv', index=False)\n",
    "\n",
    "# Log di controllo\n",
    "print(f\"\\n=== CONVERSIONE 'featured_artists' COMPLETATA ===\")\n",
    "print(f\"Totale tracce con featured artists: {len(feat_tracks)}\")\n",
    "print(f\"Tracce senza featured artists: {(tracks['featured_artists'].apply(len) == 0).sum()}\")\n",
    "print(\"\\nEsempi di conversione:\")\n",
    "print(feat_tracks[['title', 'featured_artists']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53886c77",
   "metadata": {},
   "source": [
    "### FIX THE SONGS WITHOUT FEATURED ARTIST IN THE SPECIFIC COLUMN BUT WITH FEAT ARTIST IN THE TITLE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8add859b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Totale tracce senza featured artists nella colonna: 7463\n",
      "\n",
      "=== DATAFRAME FEATURING ESTRATTI ===\n",
      "\n",
      "Totale tracce con featuring estratti: 406\n"
     ]
    }
   ],
   "source": [
    "tracks['featured_artists'] = tracks['featured_artists'].apply(\n",
    "    lambda x: x if isinstance(x, list) else ([] if pd.isna(x) else [x])\n",
    ")\n",
    "\n",
    "tracks_no_explicit_feat = tracks[tracks['featured_artists'].apply(len) == 0]\n",
    "print(f\"\\nTotale tracce senza featured artists nella colonna: {len(tracks_no_explicit_feat)}\")\n",
    "\n",
    "has_amp_after_by = tracks_no_explicit_feat['full_title'].str.contains(r'\\bby\\b[^&]*&', case=False, na=False, regex=True)\n",
    "\n",
    "tracks_by_amp = tracks_no_explicit_feat[has_amp_after_by].copy()\n",
    "\n",
    "# Conta quanti \"&\" ci sono dopo \"by\"\n",
    "def count_ampersands_after_by(title):\n",
    "    \"\"\"Conta quanti '&' ci sono dopo 'by' nel titolo\"\"\"\n",
    "    match = re.search(r'\\bby\\b(.+)', title, re.IGNORECASE)\n",
    "    if match:\n",
    "        after_by = match.group(1)\n",
    "        return after_by.count('&')\n",
    "    return 0\n",
    "\n",
    "tracks_by_amp['n_ampersands'] = tracks_by_amp['full_title'].apply(count_ampersands_after_by)\n",
    "\n",
    "\n",
    "# Creazione dataframe featuring estratti da \"by nome & nome2 & nome3\"\n",
    "\n",
    "def extract_featuring_names(title):\n",
    "    \"\"\"Estrae i nomi degli artisti dopo 'by' separati da '&'\"\"\"\n",
    "    match = re.search(r'\\bby\\b\\s*(.+)', title, re.IGNORECASE)\n",
    "    if match:\n",
    "        after_by = match.group(1).strip()\n",
    "        # Pulisce eventuali parentesi finali e altri caratteri\n",
    "        after_by = re.sub(r'\\s*[\\(\\[\\]].*$', '', after_by)\n",
    "        # Splitta per '&' e pulisce gli spazi\n",
    "        names = [name.strip() for name in after_by.split('&')]\n",
    "        return names\n",
    "    return []\n",
    "\n",
    "# Applica l'estrazione solo alle tracce con & dopo by\n",
    "tracks_by_amp['featuring_list'] = tracks_by_amp['full_title'].apply(extract_featuring_names)\n",
    "\n",
    "\n",
    "\n",
    "# Gestisco il caso in cui si hanno feat scritti nel modo: by Lazza, Murda, Beny Jr, Guy2Bezbar & Elias\n",
    "# attualmente nel vettore dei feature prendo quello a destra del & fino al by e quello dopo il &\n",
    "# esempio: ['Lazza, Murda, Beny Jr, Guy2Bezbar', 'Elias']\n",
    "# devo ridividere quelli associati con la , in elementi singoli del vettore\n",
    "\n",
    "# Splitta anche per virgole oltre che per &\n",
    "def split_featuring_names(names_list):\n",
    "    \"\"\"Splitta ulteriormente i nomi che contengono virgole\"\"\"\n",
    "    final_names = []\n",
    "    for name in names_list:\n",
    "        # Se il nome contiene virgole, splittalo\n",
    "        if ',' in name:\n",
    "            # Splitta per virgola e aggiungi ogni pezzo\n",
    "            final_names.extend([n.strip() for n in name.split(',')])\n",
    "        else:\n",
    "            final_names.append(name.strip())\n",
    "    return final_names\n",
    "\n",
    "# Applica lo split delle virgole\n",
    "tracks_by_amp['featuring_list'] = tracks_by_amp['featuring_list'].apply(split_featuring_names)\n",
    "\n",
    "\n",
    "# Crea dataframe con una riga per ogni combinazione canzone-featuring\n",
    "feat_data = []\n",
    "for idx, row in tracks_by_amp.iterrows():\n",
    "    names = row['featuring_list']\n",
    "    if len(names) > 0:\n",
    "        feat_data.append({\n",
    "            'track_id': idx,\n",
    "            'name_artist': row['name_artist'],\n",
    "            'full_title': row['full_title'],\n",
    "            'featuring_names': names,\n",
    "            'n_featuring': len(names)\n",
    "        })\n",
    "\n",
    "# levo dai vettori nella colonna featuring_names le eventuali occorrenze del name_artist\n",
    "# Normalizza gli spazi nei featuring (rimuove \\xa0 e altri spazi speciali)\n",
    "for entry in feat_data:\n",
    "    artist_name_lower = entry['name_artist'].strip().lower()\n",
    "    # Normalizza solo i nomi nei featuring e confronta con l'artista principale\n",
    "    entry['featuring_names'] = [\n",
    "        name.replace('\\xa0', ' ').strip() for name in entry['featuring_names'] \n",
    "        if name.replace('\\xa0', ' ').strip().lower() != artist_name_lower\n",
    "    ]\n",
    "\n",
    "df_featuring = pd.DataFrame(feat_data)\n",
    "\n",
    "print(f\"\\n=== DATAFRAME FEATURING ESTRATTI ===\\n\")\n",
    "print(f\"Totale tracce con featuring estratti: {len(df_featuring)}\")\n",
    "\n",
    "\n",
    "# Salva il dataframe (opzionale)\n",
    "# df_featuring.to_csv('./data/temp/feats.csv', index=False)\n",
    "\n",
    "# === AGGIORNA IL DATAFRAME ORIGINALE CON I NUOVI FEATURING ESTRATTI ===\n",
    "\n",
    "tracks['featured_artists'] = tracks['featured_artists'].apply(\n",
    "    lambda x: x if isinstance(x, list) else ([] if pd.isna(x) else [x])\n",
    ")\n",
    "\n",
    "# Series con track_id → lista featuring\n",
    "feat_map = df_featuring.set_index('track_id')['featuring_names']\n",
    "\n",
    "# Aggiorna solo le righe i cui indici sono presenti in feat_map\n",
    "tracks.loc[feat_map.index, 'featured_artists'] = feat_map\n",
    "\n",
    "tracks.to_csv('./data/temp/tracks_cleaned.csv', sep=',')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa104248",
   "metadata": {},
   "source": [
    "### FIX SOME ACTIVE_START IN THE DATASET ARTIST TAKING FIRST SONG OR FIRST FEAT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d1fff28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad rows before update: 0\n",
      "Bad rows after update:  0\n",
      "Newly bad rows introduced: 0\n",
      "Old bad rows still bad:    0\n",
      "Good rows still good:      104\n",
      "Percentage of dataset that became bad: 0.00%\n",
      "Percentage of dataset that remains bad: 0.00%\n",
      "Righe che hanno ricevuto un active_start imputato: 53\n",
      "\n",
      "Gli artisti rimasti senza active_start valido dopo l'imputazione sono:\n",
      " id_author\n",
      "ART42220690    o zulù\n",
      "Name: name, dtype: object\n",
      "\n",
      "=== AGGIORNAMENTO DATASET ARTISTS ===\n",
      "Dataset artists aggiornato con 53 nuovi valori di active_start\n",
      "Tipo di dato active_start: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "# Copia del dataframe artists\n",
    "artists_compare = artists.copy()\n",
    "\n",
    "# Converto active_start e birth_date in anno intero\n",
    "artists_compare['active_start'] = pd.to_datetime(\n",
    "    artists['active_start'], errors='coerce'\n",
    ").dt.year\n",
    "artists_compare['birth_date'] = pd.to_datetime(\n",
    "    artists['birth_date'], errors='coerce'\n",
    ").dt.year\n",
    "\n",
    "# Assicurati che 'year' sia numerico\n",
    "tracks['year'] = pd.to_numeric(tracks['year'], errors='coerce')\n",
    "\n",
    "# Esplodi la colonna featured_artists (se è già lista)\n",
    "exploded = tracks.explode('featured_artists').rename(columns={'featured_artists': 'featured_artist'})\n",
    "\n",
    "# Rimuovi eventuali NaN\n",
    "exploded = exploded[exploded['featured_artist'].notna()]\n",
    "\n",
    "# Normalizza i nomi\n",
    "exploded['featured_artist'] = exploded['featured_artist'].str.strip().str.lower()\n",
    "artists_name_lower = artists_compare['name'].str.lower()\n",
    "\n",
    "# Trova il primo anno come artista principale\n",
    "main_years = tracks.groupby('id_artist')['year'].min()\n",
    "\n",
    "# Trova il primo anno come artista featured\n",
    "featured_years = exploded.groupby('featured_artist')['year'].min()\n",
    "\n",
    "# Mappa gli anni principali e featured agli artisti\n",
    "main_years_mapped = pd.Series(artists_compare.index.map(main_years), index=artists_compare.index)\n",
    "featured_years_mapped = pd.Series(artists_name_lower.map(featured_years), index=artists_compare.index)\n",
    "\n",
    "# Combina e riempi active_start\n",
    "artists_compare['active_start'] = artists_compare['active_start'].fillna(\n",
    "    main_years_mapped.combine_first(featured_years_mapped)\n",
    ")\n",
    "\n",
    "# Converti in intero\n",
    "artists_compare['active_start'] = artists_compare['active_start'].astype('Int64')\n",
    "\n",
    "# Funzione per identificare valori \"sbagliati\"\n",
    "def is_bad(row):\n",
    "    if pd.isna(row['active_start']) or pd.isna(row['birth_date']):\n",
    "        return False\n",
    "    return row['active_start'] <= row['birth_date']\n",
    "\n",
    "# Analisi prima e dopo\n",
    "artists_before = artists.copy()\n",
    "artists_before['active_start'] = pd.to_datetime(artists_before['active_start'], errors='coerce').dt.year\n",
    "artists_before['birth_date'] = pd.to_datetime(artists_before['birth_date'], errors='coerce').dt.year\n",
    "artists_before['bad'] = artists_before.apply(is_bad, axis=1)\n",
    "artists_compare['bad'] = artists_compare.apply(is_bad, axis=1)\n",
    "\n",
    "num_bad_before = artists_before['bad'].sum()\n",
    "num_bad_after = artists_compare['bad'].sum()\n",
    "new_bad = ((~artists_before['bad']) & (artists_compare['bad'])).sum()\n",
    "old_bad_still_bad = ((artists_before['bad']) & (artists_compare['bad'])).sum()\n",
    "good_still_good = ((~artists_before['bad']) & (~artists_compare['bad'])).sum()\n",
    "\n",
    "total = len(artists)\n",
    "\n",
    "# Conteggio righe che hanno ricevuto un active_start imputato\n",
    "missing_before = pd.to_datetime(artists['active_start'], errors='coerce').isna()\n",
    "filled_now = missing_before & artists_compare['active_start'].notna()\n",
    "num_imputed = filled_now.sum()\n",
    "\n",
    "print(f\"Bad rows before update: {num_bad_before}\")\n",
    "print(f\"Bad rows after update:  {num_bad_after}\")\n",
    "print(f\"Newly bad rows introduced: {new_bad}\")\n",
    "print(f\"Old bad rows still bad:    {old_bad_still_bad}\")\n",
    "print(f\"Good rows still good:      {good_still_good}\")\n",
    "print(f\"Percentage of dataset that became bad: {100 * new_bad / total:.2f}%\")\n",
    "print(f\"Percentage of dataset that remains bad: {100 * old_bad_still_bad / total:.2f}%\")\n",
    "print(f\"Righe che hanno ricevuto un active_start imputato: {num_imputed}\")\n",
    "print(f\"\\nGli artisti rimasti senza active_start valido dopo l'imputazione sono:\\n {artists_compare.loc[artists_compare['active_start'].isna(), 'name']}\")\n",
    "\n",
    "# === APPLICA LE MODIFICHE AL DATASET ARTISTS ORIGINALE ===\n",
    "print(\"\\n=== AGGIORNAMENTO DATASET ARTISTS ===\")\n",
    "\n",
    "# Converti active_start in formato datetime con anno (mantieni il formato originale)\n",
    "# Usa il formato 'YYYY-01-01' per mantenere la compatibilità\n",
    "artists['active_start'] = artists_compare['active_start'].apply(\n",
    "    lambda x: pd.Timestamp(year=int(x), month=1, day=1) if pd.notna(x) else pd.NaT\n",
    ")\n",
    "\n",
    "print(f\"Dataset artists aggiornato con {num_imputed} nuovi valori di active_start\")\n",
    "print(f\"Tipo di dato active_start: {artists['active_start'].dtype}\")\n",
    "\n",
    "# Salva il dataframe finale (temporaneo)\n",
    "artists.to_csv('./data/temp/artists_cleaned.csv', sep=',', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9525aa54",
   "metadata": {},
   "source": [
    "### CHECK IF ALL THE ARTISTS HAVE AT LEAST ONE SONG IN THE TRACKS DATASET AFTER THE CLEANING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b4e526a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Controllo artisti con tracce associate ===\n",
      "Totale artisti nel dataset: 104\n",
      "Totale artisti con almeno una traccia (main): 104\n",
      "Totale artisti che appaiono come featuring: 85\n",
      "Totale artisti con almeno una traccia (main + featuring): 104\n",
      "Artisti con almeno una traccia: 104\n",
      "Artisti senza tracce: 0\n",
      "=== Controllo completato ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def check_artists_have_tracks(artists_df, tracks_df):\n",
    "    \"\"\"\n",
    "    Controlla quali artisti hanno almeno una traccia nel dataset tracks.\n",
    "    Aggiorna direttamente il dataframe artists_df aggiungendo una colonna 'has_tracks'.\n",
    "    Controlla sia come artista principale (id_artist) che come featuring (featured_artists).\n",
    "    \"\"\"\n",
    "    print(\"=== Controllo artisti con tracce associate ===\")\n",
    "    total_artists = len(artists_df)\n",
    "    print(f\"Totale artisti nel dataset: {total_artists}\")\n",
    "\n",
    "    # Set degli ID artisti presenti in tracks come artisti principali\n",
    "    main_artists = set(tracks_df['id_artist'].dropna().unique())\n",
    "    \n",
    "    # Set dei nomi artisti presenti in tracks come featuring (esplodendo le liste)\n",
    "    feat_artists = set()\n",
    "    for feat_list in tracks_df['featured_artists'].dropna():\n",
    "        if isinstance(feat_list, list):\n",
    "            feat_artists.update([name.strip().lower() for name in feat_list])\n",
    "    \n",
    "    # Mappa nome → id artista\n",
    "    name_to_id = dict(zip(artists_df['name'].str.lower(), artists_df.index))\n",
    "    \n",
    "    # ID degli artisti che appaiono come featuring\n",
    "    feat_artist_ids = {name_to_id[name] for name in feat_artists if name in name_to_id}\n",
    "    \n",
    "    # Combina artisti principali e featuring\n",
    "    artists_with_tracks = main_artists | feat_artist_ids\n",
    "    \n",
    "    print(f\"Totale artisti con almeno una traccia (main): {len(main_artists)}\")\n",
    "    print(f\"Totale artisti che appaiono come featuring: {len(feat_artist_ids)}\")\n",
    "    print(f\"Totale artisti con almeno una traccia (main + featuring): {len(artists_with_tracks)}\")\n",
    "\n",
    "    # Crea colonna booleana 'has_tracks'\n",
    "    artists_df['has_tracks'] = artists_df.index.isin(artists_with_tracks)\n",
    "\n",
    "    # Conta quanti artisti hanno tracce e quanti no\n",
    "    num_with_tracks = artists_df['has_tracks'].sum()\n",
    "    num_without_tracks = total_artists - num_with_tracks\n",
    "    print(f\"Artisti con almeno una traccia: {num_with_tracks}\")\n",
    "    print(f\"Artisti senza tracce: {num_without_tracks}\")\n",
    "\n",
    "    # Mostra alcuni esempi di artisti senza tracce\n",
    "    if num_without_tracks > 0:\n",
    "        print(\"\\nEsempi di artisti senza tracce associate:\")\n",
    "        print(artists_df.loc[~artists_df['has_tracks'], ['name']].head(10))\n",
    "\n",
    "    print(\"=== Controllo completato ===\\n\")\n",
    "    return artists_df\n",
    "\n",
    "# Esegui il controllo\n",
    "artists = check_artists_have_tracks(artists, tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611fd1bf",
   "metadata": {},
   "source": [
    "## SAVE CHANGES ON CSV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ca4301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save artists dataset\n",
    "# output_path = f'{data}artists.csv'\n",
    "# artists.to_csv(output_path, sep=',')\n",
    "# print(f'Dataset artists salvato in: {output_path}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a109afb",
   "metadata": {},
   "source": [
    "### TRACKS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b405dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset tracks salvato in: ./data/cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# # Save tracks dataset\n",
    "# output_path = f'{data}tracks.csv'\n",
    "# tracks.to_csv(output_path, sep=',')\n",
    "# print(f'Dataset tracks salvato in: {output_path}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de9042",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
